
** Big file systems 

** Case study: GFS 

*** Google File System (GFS)
 Design driven by application workload and technological environment 
 - Component failures are the norm rather than exception 
 - Hundreds/thousands of “file servers”, hundreds of terabytes, inexpensive HW, similar number of clients 
 - Huge files – typically multi-GB per file; data sets of many terabytes
 - File contains many application objects (e.g., Web page)
 - File mutation by writing/appending data
 - Random writes rare, usually write-once, read-sequentially 
 - E.g., files used as buffer in multi-producer/single-consumer patterns 
 - Optimization of appending crucial, as are atomicity guarantees
 - Can co-design file system and application 
 - Relaxed consistency model
 - No legacy API, new functions like atomic append 
 Check for details: S. Ghemawat, H. Gobioff, S.-T. Leung, The Google File System, Proc. SOSP 2003
*** GFS design assumptions 
 Frequently failing, inexpensive commodity components
 - Modest number (a few million) of large files (typically at least 100 MB)
 - No need to optimize for small files
 - Workload
 - Large streaming reads: read 100s KBs (usually much more); often successively read through entire file
 - Small random reads: a few KBs at some arbitrary offset 
 - Large, sequential writes appending to file (similar sizes) 
 - Random writes very rare, ok if inefficient
 - Multiple clients concurrently appending to same file is common, needs well defined semantics (atomicity for each append) 
 - High sustained bandwidth more important than small delay 
 - API: create, delete, open, close, read, write; and: snapshot, record append 
*** GFS architecture 
 GFS cluster: 1 master, multiple chunkservers, multiple clients 
 - User-level process on commodity Linux machine 
 - Files: sequence of fixed-size chunks, hierarchical namespace 
 - Chunk: 
 - Has immutable, globally unique 64-bit identifier – chunk handle – assigned by master at chunk creation time 
 - Version number, to detect stale copies (chunkserver down, update missed)
 - Typical size: 64 MB 
 - Chunk stored as normal Linux file by chunkserver
 - Chunks replicated on multiple chunkservers 
 - Default: three copies 
 - Master: Stores all file system metadata
 - Namespace, access control, mapping files ! chunks, current location of chunks 
 - Periodically talk to all chunkservers 
 - Elected via the Chubby lock service (compare later) 
*** GFS architecture overview

 S. Ghemawat, H. Gobioff, S.-T. Leung, The Google File System, Proc. SOSP 2003
*** Single master 
 Single master simplifies design, but must not become bottleneck 
 - Example: read
 - Client ask master which chunkserver to contact for a given file/byte range 
 - Master provides this metadata (all replicas for chunk), client caches it
 - Actual read directly from client to chunkserver (client chooses one replica)
 - Metadata
 - File & chunk namespaces 
 - Mapping from files to chunks
 - Locations of each chunk’s replicas 
 - All kept in memory of master; 1+2 made persistent by keeping a log; 3 is generated by asking all chunkservers at master startup 
 - 
*** Consistency model 
 File namespace mutations (e.g., file creation) are atomic
 - Handled exclusively by single master
 - Regions of files are unit of consistency
 - Possible states for a region 
 - Consistent: all clients always see the same data 
 - (regardless of used replica) 
 - Defined (with respect to a file mutation): consistent, 
 - and all clients see the entire result of the file mutation 
 - E.g., a write succeeds and is not interfered 
 - with by concurrent writes 
 - Inconsistent: a mutation on the region failed (aborted, …) 
 - Concurrent mutations? 
 - If all succeed, file region is consistent, but usually not defined  
 - Contains fragments from multiple mutations 
 - How to achieve consistency? 
 - Apply all mutations to all replicas in the same order – esp. attractive for appends! 
 - Use chunk version numbers 
 - 
 Primary	  Replica 






























*** Consistent mutations
 Achieved by leases: master grants chunk lease to one replica, turning it into the primary replica 
 - Primary replica picks mutation order for all concurrent mutations to the chunk 
 - Obeyed by all other chunkservers storing replicas of the chunk 
 Steps
 - Client asks master for locations of all replicas and primary chunkserver
 - Master replies
 - Client pushes mutations to all replicas, in any order (only data, not control flow!); stored in buffer
 - Upon receiving acks from all replicas, client sends write request to primary 
 - Primary chooses serial number for this request, distributes it to all secondaries 
 - Secondaries ack the actual write
 - Primary acks to the client
*** Atomic record append 
 Record append: Data is written to the file, but offset where is chosen by GFS 
 - Due to possibly multiple concurrent appends 
 - Guarantee: Data will be appended at least once 
 - Atomic record append possible with previous control flow 
 - Since primary replica serves as natural serialization point 
 - 
*** GFS: Some numbers 



** Case study: HDFS 

*** The clone: Hadoop Distributed File System 
 24
 Check for details: K.Shvachko, H.Kuang, S. Radia, R. Chansler, The Hadoop Distributed File System, Proc. IEEE MSST, 2010
 Block locations are exposed to clients; allows scheduling jobs 
 - Replica management aware of racks 
 - Read and writes largely similar to GFS 
 - Useful: HDFS can be mounted like traditional FSs 


